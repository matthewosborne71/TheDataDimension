{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction in Python\n",
    "\n",
    "In this lecture we will talk about two techniques that can be used to reduce the dimension of your data set: PCA and t-SNE.\n",
    "\n",
    "## Why Dimensionality Reduction?\n",
    "\n",
    "In a number of situations we would be fine without reducing the dimension, for instance consider our classic iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "iris = sns.load_dataset(\"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris, hue = \"species\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data set the dimension is manageable. We are examining 2-dimensional slices of a 4-dimensional data set. We can wrap our 3-dimensional brains around that. However, let's consider the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gray()\n",
    "plt.matshow(digits.images[440])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set has many more dimensions, if we wanted to make a pairs plot of this data it would be 64 by 64 and essentially unintelligible. A data reduction could be incredibly useful here.\n",
    "\n",
    "Dimensionality reduction is useful for both data analysis/visualization and prediction tasks. Let's dive in\n",
    "\n",
    "## Principal Components Analysis (PCA)\n",
    "\n",
    "This technique has been around since Karl Pearson's 1903 paper, <a href = \"https://www.tandfonline.com/doi/abs/10.1080/14786440109462720\"><i>On lines and planes of closest fit to systems of points in space</i></a>. It is well understood and, because of that, very popular. We'll first cover the idea behind PCA, then give a brief outline of its mathematical formulation.\n",
    "\n",
    "### What is PCA anyway?\n",
    "\n",
    "This explanation follows the reference: https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf.\n",
    "\n",
    "#### Maximum Information Retention\n",
    "\n",
    "When you reduce the dimension of a data set you are inherently losing information. Therefore when you reduce the dimension you want to ensure that you do it in a way that \"retains as much information as possible\". PCA tackles this problem in a very statistical manner.\n",
    "\n",
    "There's an idea in statistics that the information of a data set is located within that data set's variation. Thus when you reduce the dimension of a data set, you want to project your data onto a hyperplane that captures as much of the original variance in the data as possible. Thinking in terms of optimization, your goal is to project into a lower dimensional hyperplane in a way that maximizes variance.\n",
    "\n",
    "Here's a heuristic algorithm:\n",
    "1. Center your data so that each feature has 0 mean.\n",
    "2. Find the direction in space along which projections have the highest variance, this produces the first principal component.\n",
    "3. Find the direction orthogonal to the first principal component that maximizes variance, this is the second principal component.\n",
    "4. Continue in this way, the kth principal component is the variance-maximizing direction orthogonal to the previous k-1 components.\n",
    "\n",
    "Let's see what we mean in a 2-D example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first principal component is found along the longer arrow, and the second along the shorter arrow.\n",
    "\n",
    "So how can we go about projecting and maximizing variance in this way?\n",
    "\n",
    "#### Minimal Perpendicular Lengths\n",
    "\n",
    "One way to maximize variance is to minimize the collective length of the perpendiculars from the observed data points to the fitted hyperplane. This was actually Pearson's original formulation of the problem, as his goal was to contemplate what we mean by line (or plane) of best fit. Consider this image from Pearson's 1903 paper.\n",
    "\n",
    "<img src = \"PearsonLine.png\" width = \"500\"></img>\n",
    "\n",
    "Recall that the Mean Square Error is given by the following:\n",
    "$$\n",
    "MSE = 1/n\\sum_{i = 1}^{n} ||\\text{observed point }i - \\text{fit of point }i ||^2\n",
    "$$\n",
    "Where we are assuming $n$ total observations and by fit we mean the projection of observation $i$ onto the hyperplane. It can be mathematically shown that minimizing the $MSE$ is equivalent to maximizing the variance!\n",
    "\n",
    "We will not go through the details here.\n",
    "\n",
    "#### Eigen Data\n",
    "\n",
    "Now from the derivation of that fact a constrained optimization problem can be set up, in which the end result shows that the principal components come from the eigenvectors of the covariance matrix of the original data. The first principal component is the eigenvector corresponding to the largest eigenvalue, the second principal component is the eigenvector corresponding to the second largest eigenvalue and so on.\n",
    "\n",
    "This was just a quick rundown of the mathematics/statistics behind PCA. For more information please read through the two resources I have linked to above. For the rest of the PCA section we will actually implement PCA and see what it can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing PCA\n",
    "\n",
    "Let's use PCA and see how powerful it can be.\n",
    "\n",
    "#### The PC(ancer)A\n",
    "\n",
    "We'll start by showing how PCA can help identify benign and malignant cancer using the `sklearn` cancer data set. Note, this example comes from the book, <u>Introduction to Machine Learning with Python</u>, by Andreas C. M&uuml;ller & Sarah Guido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "malignant = cancer.data[cancer.target == 0]\n",
    "benign = cancer.data[cancer.target == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(15, 2, figsize = (10,20))\n",
    "\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i in range(30):\n",
    "    _, bins = np.histogram(cancer.data[:, i], bins = 50)\n",
    "    ax[i].hist(malignant[:, i], bins = bins, color = 'red', alpha = .5)\n",
    "    ax[i].hist(benign[:, i], bins = bins, color = 'blue', alpha = .5)\n",
    "    ax[i].set_title(cancer.feature_names[i])\n",
    "    ax[i].set_yticks(())\n",
    "\n",
    "ax[0].legend(['malignant', 'benign'], loc = 'best')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are definitely some differences between malignant and benign cancer, however, determining cutoffs or decision rules could prove troublesome with 30 different variables to consider. Let's see if PCA with 2 components can help.\n",
    "\n",
    "Note first we have to preprocess the data a little and scale the variables, otherwise a variable like worst area will have more impact on the variance than variables like worst symmetry and worst fractal dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaling the data, standard scaler scales the data \n",
    "# like a standard normal rv, this may not be the best way\n",
    "# to scale the data though\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(cancer.data)\n",
    "X_scaled = scaler.transform(cancer.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we do PCA\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# Fit the PCA model to the scaled data\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Transform the data, i.e. project onto the fitted hyperspace\n",
    "X_pca = pca.transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we plot and see what happened:\n",
    "cancer_pca = pd.DataFrame({'pca_1':X_pca[:,0],'pca_2':X_pca[:,1],'cancer_outcome':cancer.target})\n",
    "\n",
    "sns.lmplot(data = cancer_pca, x = 'pca_1', y = 'pca_2', hue = 'cancer_outcome', \n",
    "           fit_reg = False, legend = False, height = 10, aspect = 1)\n",
    "\n",
    "plt.legend(cancer.target_names)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see PCA does a pretty good job capturing the split between benign and malignant tumors with only 2 dimensions!\n",
    "\n",
    "This is wonderful, but you might have a couple of questions:\n",
    "\n",
    "1. How can I know how many dimensions are \"enough\"?\n",
    "2. Is there a way to interpret this data in a way that I can relate back to the original problem?\n",
    "\n",
    "We'll start to answer both of those now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does it all mean?\n",
    "\n",
    "A clear downside of PCA is that it isn't very easy to interpret. For instance it isn't clear from our cancer plot above what from the data leads to an increase or decrease in the x and y directions. How can we interpret what separates benign and malignant tumors from this technique?\n",
    "\n",
    "One way we can attempt to interpret the results is to look at the principal components themselves. Principal components correspond to directions in the original data, so they are combinations of the original features. In some instances we may be able to gleam some insights from the components, however, these can also be quite complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_comps = pd.DataFrame({'Feature':cancer.feature_names,\n",
    "                          'First Component':pca.components_[0,:],\n",
    "                          'Second Component':pca.components_[1,:]})\n",
    "\n",
    "pca_comps = pca_comps.sort_values(['Second Component'])\n",
    "\n",
    "pca_comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize with a heatmap\n",
    "plt.matshow(pca.components_, cmap = 'viridis')\n",
    "\n",
    "plt.yticks([0,1], [\"First Component\", \"Second Component\"],\n",
    "          fontsize = 14)\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xticks(range(len(cancer.feature_names)),\n",
    "          cancer.feature_names, rotation = 90, \n",
    "          fontsize = 14)\n",
    "\n",
    "plt.xlabel(\"Feature\", fontsize = 14)\n",
    "plt.ylabel(\"Principal Components\", fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try and interpret the results. Recall that the first component gives us the x-coordinate on our projection plot above. The first component from the PCA is positive for all features. This means that an increase in any one of the features causes an increase in the first component. As we can see from our plot this means that a large value for any one of the features is likely not a good sign, because the right half of the project plot is dominated by malignant observations.\n",
    "\n",
    "In the second component we have some differences in signs. This can help us identify which features lead to an increase or decrease in our y-coordinate. For instance an increase in mean radius pushes our projection point down in the plot while an increase in mean fractal dimension would push our point up in the plot.\n",
    "\n",
    "Combining our knowledge of the components with the projection plot can allow us to better understand what features of the data coincide with a malignant observation and what features coincide with a benign observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explained Variance Ratio - How many dimensions?\n",
    "\n",
    "How many dimensions do I need to project down to? Well that depends upon the problem at hand, but `sklearn`'s PCA function does give us the tools to figure out how much of the variance each principal component explains. This is known as the explained variance ratio of the component and is available via the explained_variance_ratio_ variable. These values indicate the proportion of the dataset's variance that lies along the axis of each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outcome tells us that 44.27% of the variance in the original data is contained in the first component and 18.97% in the second component. Perhaps we'd like more variance, or less. We can see how much of the variance we capture in total by taking the cumulative sum of the ratios for each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 30)\n",
    "\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(range(1,31), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.axis((1,30,0,1.1))\n",
    "plt.title(\"Explained Variance Ratio for Cancer Data\",fontsize = 16)\n",
    "plt.xlabel(\"Dimensions\",fontsize = 14)\n",
    "plt.ylabel(\"Explained Variance\", fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice feature of `sklearn`'s PCA is that you can tell it what you want the cumulative explained variance ratio to be, and then it will choose the minimum number of components that meat that specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's an example where we set it at .85\n",
    "pca = PCA(n_components = .85)\n",
    "\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One rule of thumb for choosing the dimension of the hyperplane is to look for the \"elbow\" in the cumulative explained variance plot. This is where the explained variance ratio stops growing as quickly and is thought of as the \"intrinsic dimensionality\" of the dataset. This looks to be at around 6 or 7 dimensions for the cancer data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a feel for PCA. Before we move on we'll do a few more interesting examples of applying PCA.\n",
    "\n",
    "### P(olitical)CA\n",
    "\n",
    "Political affiliations result in a very natural classification problem. There are clearly marked political affiliations, Republican, Democrat, Independent. As it turns out we can use PCA to help take rather large datasets and project down onto two dimensions that capture political affiliation quite well.\n",
    "\n",
    "#### Roll Call Classifications\n",
    "\n",
    "This section was inspired from a talk given by <a href = \"https://www.math.ucla.edu/~mason/\">Mason Porter</a>, for reference see the two papers he has published <a href = \"https://www.sciencedirect.com/science/article/pii/S0378437107007844\">here</a> and <a href = \"https://www.pnas.org/content/102/20/7057.short\">here</a>. \n",
    "\n",
    "Every Congress, congressman must meet and cast votes on various bills. Congressman can vote 'Yea', 'Nay', 'Abstain' or be absent. From this information you can construct a matrix for every single session of congress where each row is a congressman and each column is a particular roll call vote. Every entry in this matrix is a $1$ if the congressman voted 'Yea', a $-1$ if the congressman voted 'Nay', and $0$ otherwise.\n",
    "\n",
    "Below we will see what happens when we put such a matrix through PCA with 2 components.\n",
    "\n",
    "All data was taken from <a href = \"https://voteview.com/data\">voteview.com</a>, and then cleaned to be in the presented format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll look at the 112th Senate\n",
    "congress_112 = pd.read_csv(\"Senate_112_roll_mat.csv\")\n",
    "\n",
    "congress_112.loc[congress_112.party_code == 200, \"party_code\"] = \"Republican\"\n",
    "congress_112.loc[congress_112.party_code == 100, \"party_code\"] = \"Democrat\"\n",
    "congress_112.loc[congress_112.party_code == 328, \"party_code\"] = \"Independent\"\n",
    "\n",
    "congress_112.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the first three columns of the dataframe are identifiers so we know who cast the vote. The name is the name, the icpsr is the identifier code for that congressman in that particular congress, and the party_code is a code given for the political affiliation of the congressman (100 is Democrat, 200 is Republican, and 328 is Independent). The remaining columns each represent a bill that was voted on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_votes = len(congress_112.columns) -3\n",
    "\n",
    "# Retrieve Just the Votes\n",
    "X = congress_112[list(map(str,range(1,total_votes + 1)))]\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "fit = pca.fit_transform(X)\n",
    "\n",
    "congress_112_pca = congress_112[['name','icpsr','party_code']].copy()\n",
    "congress_112_pca['pca_x'] = fit[:,0]\n",
    "congress_112_pca['pca_y'] = fit[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "\n",
    "sns.lmplot(data = congress_112_pca, \n",
    "           x = 'pca_x', \n",
    "           y = 'pca_y', \n",
    "           hue = 'party_code', \n",
    "           palette = {\"Democrat\":'blue', \"Republican\":'red', \"Independent\": 'purple'},\n",
    "           fit_reg = False, \n",
    "           height = 7, aspect = 1, \n",
    "           scatter_kws = {'s':15})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this might not seem too impressive, way to go we recovered that there are two political parties. But think about what we did. We took information stored in 486 columns, compressed it to two columns and still retained meaningful information.\n",
    "\n",
    "Heck we could even get away with one dimension it seems. Let's examine what it looks like with just the first principal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_112_pca['Dummy'] = \"\"\n",
    "\n",
    "sns.stripplot(data = congress_112_pca, \n",
    "              x = 'pca_x',\n",
    "              y = 'Dummy',\n",
    "              hue = 'party_code',\n",
    "              palette = {\"Democrat\":'blue', \"Republican\":'red', \"Independent\": 'purple'}, \n",
    "              jitter = 0)\n",
    "\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the sole independent does get swallowed up here, we see that we only need one dimension to explain the party split. There really isn't much variation in the way our congressman vote. \n",
    "\n",
    "A fun fact, the 2-D PCA projection is how voteview.com calculates various ideological scores for Congress members. They then use those scores to make probabilstic predictions on how a congressman will vote on a particular bill. Check out voteview.com for more info!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also investigate further. For instance all of the Democrats are much more closer than the Republicans is there a reason for that? There's a cluster of three republicans in the upper right hand corner of the 2-D plot, is there a reason they're further away from the main Republican grouping?\n",
    "\n",
    "##### Practice\n",
    "\n",
    "Now you take an opportunity to practice. In the repository are data files for the 113th, 114th, and 115th Senates. Run PCA on those sets and see what you get. Is the output the same as 112th or are there differences over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csvs are stored as Senate_#_roll_mat.csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Polarization\n",
    "\n",
    "A great fear of social media is that it is leading to greater polarization of our society, but what does this have to do with PCA?\n",
    "\n",
    "Well lets look at a data set collected by the Center for the Study of Networks and Society (CSNS). For a <a href = \"http://www.susanbourbaki.com/category/breitbart/\">blog post</a> back in 2017 in which they examined the following habits of various Twitter users they collected every follower from the following 30 accounts: DrDavidDuke, AryanBrother, CofCC76, Hatchet318, KKKOfficial311, KeyStoneUnited, MatthewHeimbach, ThaRightStuff, nsm88, BreitbartNews, FiveThirtyEight, csmonitor, MotherJones, NRO, dailykos, theblaze, thenation, DRUDGE_REPORT, WSJ, washingtonpost, FoxNews, NPR, realDonaldTrump, tedcruz, BernieSanders, SenWarren, SpeakerRyan, SenJohnMcCain, marcorubio, and CoryBooker.\n",
    "\n",
    "Now we will depart from the story told by CSNS and look at a specific subset of their data, just those followers of the 13 media accounts: BreitbartNews, FiveThirtyEight, csmonitor, MotherJones, NRO, dailykos, theblaze, thenation, DRUDGE_REPORT, WSJ, washingtonpost, FoxNews, NPR. However, I encourage you to read their blog post as well as their recent paper on arxiv, <a href = \"https://arxiv.org/pdf/1905.07755.pdf\"><i>Online reactions to the 2017 ‘Unite the Right’ rally in Charlottesville: Measuring polarization in Twitter networks using media followership</i></a> that includes a more in depth analysis of this data in the context of the Charlottesville incident.\n",
    "\n",
    "Our goal will be to see if PCA can illuminate any information on the anonymized Twitter users based on their media account followership. One more additional caveat before we move forward, the original media restricted dataset contains 27,056,206 unique Twitter accounts. We will only look at a stratified sample of this data that contains only 100,000 accounts. The data has been sampled in such a way that we retain the results found by CSNS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "twitter = pd.read_csv(\"twitter_sba.csv\")\n",
    "\n",
    "twitter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now before performing PCA let's look at the believed political leanings of each of our media accounts.\n",
    "\n",
    "| Twitter Screenname | News Source               | Leaning Tendency |\n",
    "|--------------------|---------------------------|------------------|\n",
    "| FoxNews            | Fox News                  | Right            |\n",
    "| BreitbartNews      | Breitbart                 | Right            |\n",
    "| DRUDGE_REPORT      | The Drudge Report         | Right            |\n",
    "| the blaze          | TheBlaze                  | Right            |\n",
    "| NRO                | National Review           | Right            |\n",
    "| WSJ                | Wall Street Journal       | center           |\n",
    "| csmonitor          | Christian Science Monitor | left             |\n",
    "| FiveThirtyEight    | FiveThirtyEight           | left             |\n",
    "| dailykos           | Daily KOS                 | left             |\n",
    "| thenation          | The Nation                | left             |\n",
    "| MotherJones        | Mother Jones              | left             |\n",
    "| washingtonpost     | The Washington Post       | left             |\n",
    "| NPR                | NPR                       | left             |\n",
    "\n",
    "These tendencies have been corroborated by peer reviewed studies. The Wall Street Journal is labeled as center because it is widely considered to be conservative-leaning, but has been grouped as liberal based on readership and co-citations.\n",
    "\n",
    "#### Practice\n",
    "Now perform PCA and see what we get. Use 3 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do your work here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Isn't Always the Answer\n",
    "\n",
    "As we will see in the following section PCA isn't the best choice for every data compression task. Although it can be quite powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "One reason we're interested in dimensionality reduction is so we can better visualize what is going on in a high dimensional data set. As we have said before, we are 3-D creatures and so our heads can really only wrap around 2, 3 or maybe 4 dimensions. Thus it is key to have algorithms at your disposal that allow you to visualize high dimensional data in low dimensions.\n",
    "\n",
    "We'll start with an example to show how good t-SNE is at this.\n",
    "\n",
    "### Back to MNIST\n",
    "\n",
    "Recall our MNIST data that we stored in `digits`.\n",
    "\n",
    "We'll first visualize that data using PCA and then compare it to the visualization from t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(digits.data)\n",
    "pca_digits = pca.transform(digits.data)\n",
    "\n",
    "pca_digits_df = pd.DataFrame({\"Digit\":digits.target,\n",
    "                              \"pca_x\":pca_digits[:,0],\n",
    "                              \"pca_y\":pca_digits[:,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,12))\n",
    "\n",
    "for target in list(set(digits.target)):\n",
    "    temp = pca_digits_df.loc[pca_digits_df.Digit == target,]\n",
    "    plt.plot(temp.pca_x, temp.pca_y, '.', label = target, markersize = 10)\n",
    "    \n",
    "plt.legend(fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components = 2, random_state = 440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_digits = tsne.fit_transform(digits.data)\n",
    "\n",
    "tsne_df = pd.DataFrame({\"Digit\":digits.target,\n",
    "                        \"tsne_x\":tsne_digits[:,0],\n",
    "                        \"tsne_y\":tsne_digits[:,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,10))\n",
    "\n",
    "for target in list(set(digits.target)):\n",
    "    temp = tsne_df.loc[tsne_df.Digit == target,]\n",
    "    plt.plot(temp.tsne_x, temp.tsne_y, '.', label = target, markersize = 10)\n",
    "    \n",
    "plt.legend(fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, while not perfect, the t-SNE plot does a significantly better job of separating out the clusters of digits in the dataset. That's because the algorithm was designed with that in mind.\n",
    "\n",
    "\n",
    "### How Does t-SNE Work?\n",
    "\n",
    "We'll give a brief overview of how the algorithm works here. For the rigorous treatment see <a href = \"http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf\"><i>Visualizing Data using t-SNE</i></a> by Laurens van der Maaten and Geoffrey Hinton.\n",
    "\n",
    "Here's a basic outline of the algorithm:\n",
    "1. For all points $x_i$, $x_j$, convert their Euclidean distance into a conditional probability $p_{j|i}$. This is done by imagining a Gaussian distribution around $x_i$ and then comparing the \"normal distance\" of $x_j$ vs the sum of all other \"normal distances\". Here's a precise mathematical formula:\n",
    "$$\n",
    "p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2/2\\sigma_i^2)}{\\sum_{k\\neq i} \\exp(-||x_i - x_k||^2/2\\sigma_i^2)}\n",
    "$$\n",
    "Think of $p_{j|i}$ as the probability that $x_i$ would choose $x_j$ as its neighbor. We take $p_{i|i} = 0$.\n",
    "2. For every point $x_i$ in high dimensional space we will have a low dimensional counterpart, $y_i$, to which we map $x_i$. Similar to $p_{j|i}$ we will have $q_{j|i}$ that gives the probability that $y_i$ would choose $y_j$ as a neighbor. Here is the precise mathematical formula they are using:\n",
    "$$\n",
    "q_{j|i} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k\\neq l} (1 + ||y_k - y_l||^2)^{-1}}\n",
    "$$\n",
    "We take $q_{i,i} = 0$. Note: this is where the $t$ comes from! The numerator and denominator of $q_{j|i}$ is from the probability density for the $t$ distribution with 1 degree of freedom.\n",
    "3. Now if we're preserving these pairwise distances well, then we should expect $p_{j|i}$ to be close to $q_{j|i}$. In order to do this a cost function that measures the difference between $p_{j|i}$ and $q_{j|i}$ is minimized using gradient descent. The optimal $y_i$s are then spit out by the algorithm.\n",
    "\n",
    "\n",
    "You may be wondering how $\\sigma_i$ is chosen. The idea behind letting $\\sigma_i$ vary with $i$ is that some regions of the data are much denser than others. Changing $\\sigma_i$ with $i$ allows the algorithm to essentially control the number of neighbors of $x_i$ it considers. We can control this in $t-SNE$ by specifying a perplexity. Typical values for perplexity are in 5 to 50.\n",
    "\n",
    "If you'd like a more in depth non research paper explanation of t-SNE check out this blog post, http://mlexplained.com/2018/09/14/paper-dissected-visualizing-data-using-t-sne-explained/. I think the author does a great job explaining it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elevator Pitch for t-SNE\n",
    "\n",
    "Okay so maybe that was a lot of probability theory, even if it was watered down from the paper. Here's an elevator pitch. PCA is typically good at recognizing global structures of the data. t-SNE is able to detect local structures, in particular it is great at detecting clusters in the data as we saw in digits example.\n",
    "\n",
    "If you are working on a classification problem it might be a good idea to look at your data in lower dimensions in t-SNE. \n",
    "\n",
    "Let's see another example contrasting PCA and t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Blob in a Blob\n",
    "\n",
    "This will be an artificial data set of a 3-D data blobs. We'll see how bad PCA is at detecting the local structure and then contrast that performance with t-SNE. This example is from the aforementioned t-SNE blog, http://mlexplained.com/2018/09/14/paper-dissected-visualizing-data-using-t-sne-explained/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First blob\n",
    "A = np.random.normal(scale = 1, size = (300,3))\n",
    "\n",
    "# The blob that surrounds A\n",
    "B = np.array([x for x in np.random.normal(scale = 5, size = (1000,3)) if np.linalg.norm(x) > 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(A[:,0], A[:,1], A[:,2])\n",
    "ax.scatter(B[:,0], B[:,1], B[:,2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the data\n",
    "X = np.r_[A,B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First PCA\n",
    "X_PCA = PCA(n_components = 2).fit_transform(X)\n",
    "\n",
    "A_PCA = X_PCA[:A.shape[0],:]\n",
    "B_PCA = X_PCA[A.shape[0]:,:]\n",
    "\n",
    "plt.plot(A_PCA[:,0], A_PCA[:,1], '.')\n",
    "plt.plot(B_PCA[:,0], B_PCA[:,1], '.')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we didn't know that there were two blobs of data ahead of time, PCA wouldn't give us that structure. Let's examine the t_SNE now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = TSNE(n_components = 2, perplexity = 20).fit_transform(X)\n",
    "\n",
    "A_tsne = X_tsne[:A.shape[0],:]\n",
    "B_tsne = X_tsne[A.shape[0]:,:]\n",
    "\n",
    "plt.plot(A_tsne[:,0], A_tsne[:,1], '.')\n",
    "plt.plot(B_tsne[:,0], B_tsne[:,1], '.')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice\n",
    "\n",
    "Play around with the perplexity above, what happens as you make it smaller, larger?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie Genres\n",
    "\n",
    "From the following kaggle data set, https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset/downloads/imdb-5000-movie-dataset.zip/1, I extracted the movie name and the genres. From that data I created the following dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 26 possible genres, and for each genre column there is a 0 or 1 depending on whether or not that movie was classified as that genre.\n",
    "\n",
    "Using t-SNE explore if there are natural genre groupings that many films fall into. You could look at single genres like Documentary, or combinations like Fantasy Adventure. Consider adding a binary variable for a film series like Star Wars, do those films go to the same place in the plot? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe you need three spaces\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warnings About t-SNE\n",
    "\n",
    "1. The S stands for Stochastic, meaning that your results change slightly each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this 5 times\n",
    "X_tsne = TSNE(n_components = 2, perplexity = 20).fit_transform(X)\n",
    "\n",
    "A_tsne = X_tsne[:A.shape[0],:]\n",
    "B_tsne = X_tsne[A.shape[0]:,:]\n",
    "\n",
    "plt.plot(A_tsne[:,0], A_tsne[:,1], '.')\n",
    "plt.plot(B_tsne[:,0], B_tsne[:,1], '.')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. You can't really use this to make predictions on new data, unlike PCA there isn't a procedure that will map new points into the lower dimensional space. \n",
    "\n",
    "3. The magnitude of the distances between clusters shouldn't be interpreted. They are mostly a result of the t distribution.\n",
    "\n",
    "4. t-SNE results should not be used as statistical evidence or proof of something. It is not a formal statistical test.\n",
    "\n",
    "5. Use t-SNE to explore data and make hypotheses as well as cool plots!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "In this lecture we went through PCA and t-SNE. Now these are not the only two dimensionality reduction techniques, there are many more. They are two of the most popular techniques in data science so you're off to a good start.\n",
    "\n",
    "You can even combine these two techniques. Often for data with 1000s of features people will use PCA to reduce the data to the intrinsic dimensionality of the data, and then explore using t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
